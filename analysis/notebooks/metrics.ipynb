{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When fetching the ticker ES=F from Yahoo Finance, we obtain these columns:\n",
        "\n",
        "| Price | Datetime                 | Close  | High   | Low    | Open   | Volume |\n",
        "|--------|-------------------------|---------|--------|--------|--------|--------|\n",
        "| 0        | 2024-02-22 17:00:00-06:00 | 5099.00 | 5100.50 | 5094.00 | 5094.25 | 0      |\n",
        "| 1        | 2024-02-22 18:00:00-06:00 | 5095.75 | 5099.25 | 5092.25 | 5098.75 | 9420   |\n",
        "| 2        | 2024-02-22 19:00:00-06:00 | 5099.50 | 5101.75 | 5095.00 | 5095.75 | 7390   |\n",
        "| 3        | 2024-02-22 20:00:00-06:00 | 5100.25 | 5102.50 | 5099.50 | 5099.75 | 4922   |\n",
        "| 4        | 2024-02-22 21:00:00-06:00 | 5101.25 | 5102.00 | 5099.75 | 5100.50 | 4426   |\n",
        "\n",
        "The datetime format corresponds to:\n",
        "\n",
        "- **`YYYY-MM-DD`** → The date in **Year-Month-Day** format.  \n",
        "- **`HH:MM:SS`** → The time in **24-hour format** (Hour:Minute:Second).  \n",
        "- **`-06:00`** → The **timezone offset** from UTC. In this case, `UTC-6` (Central Time) since the timezone was set with `tz=\"America/Chicago\"`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Forecasting Period"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are less data points for the first two months and the last month of the year."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Q1        | Q1        | Q1     | Q2     | Q2  | Q2  | Q3  | Q3     | Q3        | Q4     | Q4      | Q4          |\n",
        "|:-----------|:-----------|:--------|:--------|:----|:----|:----|:-------|:---------|:--------|:--------|:-----------|\n",
        "| January    | February    | March   | April   | May | June | July | August | September | October | November | December  |\n",
        "|             |              | 2023   | 2023   | 2023 | 2023 | 2023 | 2023     | 2023      | 2023    | 2023     | 2023       |\n",
        "| 2024       | 2024         | 2024    | 2024    | 2024 | 2024 | 2024 | 2024     | 2024      | 2024    | 2024     | 2024 (partial) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6CZBqigaXMD"
      },
      "source": [
        "# Heatmap Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section stores constants for all sections that generate heatmap.\n",
        "There are values and labels for year, quarter, month, day, hour (+ day and night), horizon and models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "years = [2023, 2024, 2025]\n",
        "\n",
        "days = {\n",
        "    0: \"Monday\",\n",
        "    1: \"Tuesday\",\n",
        "    2: \"Wednesday\",\n",
        "    3: \"Thursday\",\n",
        "    4: \"Friday\",\n",
        "}\n",
        "day_labels = [v for k,v in days.items()]\n",
        "\n",
        "weeks = {i: i+1 for i in range(13)}\n",
        "weeks_label = [v for k,v in weeks.items()]\n",
        "\n",
        "hours = {\n",
        "    0: \"12AM\",  1: \"1AM\",  2: \"2AM\",  3: \"3AM\",  4: \"4AM\",  5: \"5AM\",\n",
        "    6: \"6AM\",  7: \"7AM\",  8: \"8AM\",  9: \"9AM\",  10: \"10AM\", 11: \"11AM\",\n",
        "    12: \"12PM\", 13: \"1PM\", 14: \"2PM\", 15: \"3PM\", 16: \"4PM\", 17: \"5PM\",\n",
        "    18: \"6PM\", 19: \"7PM\", 20: \"8PM\", 21: \"9PM\", 22: \"10PM\", 23: \"11PM\"\n",
        "}\n",
        "\n",
        "hour_labels = ['12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM', '8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM', '5PM-v', '6PM-v', '7PM-v', '8PM-v', '9PM-v', '10PM-v', '11PM-v']\n",
        "\n",
        "day_hours = {0: \"8AM\",  1: \"9AM\",  2: \"10AM\", 3: \"11AM\", 4: \"12PM\", 5: \"1PM\", 6: \"2PM\", 7: \"3PM\", 8: \"4PM\"}\n",
        "day_hours_labels = ['8AM', '9AM', '10AM', '11AM', '12PM', '1PM', '2PM', '3PM', '4PM']\n",
        "\n",
        "night_hours = {0: \"5PM\", 1: \"6PM\", 2: \"7PM\", 3: \"8PM\", 4: \"9PM\", 5: \"10PM\", 6: \"11PM\", 7: \"12AM\", 8: \"1AM\",  9: \"2AM\",  10: \"3AM\",  11: \"4AM\",  12: \"5AM\", 13: \"6AM\",  14: \"7AM\"}\n",
        "night_hours_labels = ['5PM-v', '6PM-v', '7PM-v', '8PM-v', '9PM-v', '10PM-v', '11PM-v', '12AM', '1AM', '2AM', '3AM', '4AM', '5AM', '6AM', '7AM']\n",
        "\n",
        "months = {i: i+1 for i in range(12)}\n",
        "months_label = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "\n",
        "quarters = {i: i+1 for i in range(4)}\n",
        "quarters_label = ['Q1', 'Q2', 'Q3', 'Q4']\n",
        "\n",
        "horizons = [0, 1, 2]\n",
        "horizons_label = [1, 2, 3]\n",
        "\n",
        "future = \"es\"\n",
        "models = {0: \"moirai\", 1: \"chronos\", 2: \"time_moe\"}\n",
        "models_label = [\"Moirai\", \"Chronos\", \"Time-MoE\"]\n",
        "\n",
        "context_length = 384\n",
        "context_length_index = context_length - 1\n",
        "prediction_length = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Toy function to understand how the precision is computed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code is used to explain how do we compute the signs and the predictions. It is not used in practice, it is for demonstration only.\n",
        "Also, it doesn't use the TP, FP, TN, FN columns that were added in the dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import numpy as np\n",
        "\n",
        "def precision_per_day(file_path, \n",
        "              target=\"Close\",\n",
        "              horizon=3, \n",
        "              date=None):\n",
        "\n",
        "    df = pd.read_csv(file_path, parse_dates=True, index_col=0)\n",
        "    df.dropna(inplace=True)  # remove NaN values\n",
        "    # Force index to be in datetime format\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        df.index = pd.to_datetime(df.index, utc=True)  # Convert to datetime, force UTC\n",
        "    # Convert from UTC to local time (assuming original data is in GMT-6)\n",
        "    df.index = df.index.tz_convert(\"America/Chicago\")  # Convert to Central Time\n",
        "            \n",
        "    df['Row_Position'] = range(len(df))  # Add original row position column\n",
        "\n",
        "    df_date = df.loc[date]  # Filter rows by date\n",
        "    \n",
        "    df_date_index_start = df_date.iloc[0][\"Row_Position\"]\n",
        "    df_date_index_end = df_date.iloc[-1][\"Row_Position\"]\n",
        "\n",
        "    # We take extra values (as many as there are horizons) to be able to predict the sign\n",
        "    # Correctly extract future timestamps from df.index\n",
        "    df_ground_truth_start_time = df.index[df_date_index_start + 1]\n",
        "    df_ground_truth_end_time = df.index[df_date_index_end + horizon]  # Prevent out-of-bounds\n",
        "\n",
        "    df_ground_truth = df.loc[df_ground_truth_start_time : df_ground_truth_end_time]\n",
        "\n",
        "    TP = [0 for _ in range(horizon)]\n",
        "    TN = [0 for _ in range(horizon)]\n",
        "    FP = [0 for _ in range(horizon)]\n",
        "    FN = [0 for _ in range(horizon)]\n",
        "\n",
        "\n",
        "    #print(\"\\n ---------- \\nDate: \", date)\n",
        "    #print(\"Ground truth start time: \", df_ground_truth_start_time)\n",
        "    #print(\"Ground truth end time: \", df_ground_truth_end_time)\n",
        "\n",
        "    # for each day\n",
        "    for index, row in df_date.iterrows():\n",
        "        #print(\"\\n\")\n",
        "        #print(\"Hour: \", index)\n",
        "        row_index = row[\"Row_Position\"]\n",
        "        base = row[target]\n",
        "        results_list = ast.literal_eval(row[\"Result\"])\n",
        "        \n",
        "        for horizon_index in range(1, horizon+1):\n",
        "            #print(\"Horizon n°\", horizon_index)\n",
        "            # locate the row + horizon_index \n",
        "            future_time = df.index[row_index + horizon_index]\n",
        "            y = df_ground_truth.loc[future_time, target]\n",
        "            prediction_result = results_list[horizon_index-1]\n",
        "            #print(\"Base: \", base, \"Real: \", y, \"Prediction: \", prediction_result)\n",
        "            sign_real_difference = np.sign(y - base)\n",
        "            sign_prediction_difference = np.sign(prediction_result - base)\n",
        "            #print(\"Real sign difference: \", sign_real_difference)\n",
        "            #print(\"Prediction sign difference: \", sign_prediction_difference)\n",
        "            \n",
        "            if sign_prediction_difference == 1 and sign_real_difference == 1:\n",
        "                TP[horizon_index-1] += 1\n",
        "            elif sign_prediction_difference == -1 and sign_real_difference == -1:\n",
        "                TN[horizon_index-1] += 1\n",
        "            elif sign_prediction_difference == 1 and sign_real_difference == -1:\n",
        "                FP[horizon_index-1] += 1\n",
        "            elif sign_prediction_difference == -1 and sign_real_difference == 1:\n",
        "                FN[horizon_index-1] += 1\n",
        "            else:\n",
        "                print(\"Error with signs. See the following values:\")\n",
        "                print(f\"Real: {sign_real_difference}, Prediction: {sign_prediction_difference}\")\n",
        "                print(f\"Real: {y}, Prediction: {prediction_result}\")\n",
        "    return TP, TN, FN, FP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper function to compute the TP, FP, TN, FN for every row of one dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def helper_metric(filepath: str, model: str):\n",
        "\n",
        "    df = pd.read_csv(filepath, parse_dates=True, index_col=0)\n",
        "\n",
        "    # TP: True Positive, TN: True Negative, FP: False Positive, FN: False Negative\n",
        "    df[\"TP\"] = 0\n",
        "    df[\"TN\"] = 0\n",
        "    df[\"FP\"] = 0\n",
        "    df[\"FN\"] = 0\n",
        "\n",
        "    # We take the first 384 rows as context. We start predicting from the 385th row\n",
        "    first_predicted_row = context_length_index\n",
        "    last_predicted_row = len(df) - prediction_length\n",
        "    \n",
        "    for index, row in df[first_predicted_row:last_predicted_row].iterrows():\n",
        "        base_price = row['Close']\n",
        "        future_predictions = ast.literal_eval(row[\"Result\"])\n",
        "        future_prices = df.loc[index:].iloc[1:prediction_length+1][\"Close\"].tolist() # get the next prediction_length values corresponding to the next prediction_length horizons\n",
        "        real_difference_signs = [np.sign(price - base_price) for price in future_prices]\n",
        "        predicted_difference_signs = [np.sign(prediction - base_price) for prediction in future_predictions]\n",
        "\n",
        "        TP = [0 for _ in range(prediction_length)]\n",
        "        TN = [0 for _ in range(prediction_length)]\n",
        "        FP = [0 for _ in range(prediction_length)]\n",
        "        FN = [0 for _ in range(prediction_length)]\n",
        "\n",
        "        # Compute the TP, TN, FP, FN for each horizon\n",
        "        for horizon_index, (real_difference_sign, predicted_difference_sign) in enumerate(zip(real_difference_signs, predicted_difference_signs)):\n",
        "            if real_difference_sign == predicted_difference_sign and real_difference_sign == 1:\n",
        "                TP[horizon_index] += 1\n",
        "            elif real_difference_sign == predicted_difference_sign and real_difference_sign == -1:\n",
        "                TN[horizon_index] += 1\n",
        "            elif real_difference_sign != predicted_difference_sign and real_difference_sign == 1:\n",
        "                FN[horizon_index] += 1\n",
        "            elif real_difference_sign != predicted_difference_sign and real_difference_sign == -1:\n",
        "                FP[horizon_index] += 1\n",
        "\n",
        "        # fill the column for TP, TN, FP, FN for the current row\n",
        "        df.at[index, \"TP\"] = str(TP)\n",
        "        df.at[index, \"TN\"] = str(TN)\n",
        "        df.at[index, \"FP\"] = str(FP)\n",
        "        df.at[index, \"FN\"] = str(FN)\n",
        "        \n",
        "    df.to_csv(f\"analysis/future_data/temporary_2023_2024/es_future_final_{model}_2023_2024.csv\", index=True)\n",
        "\n",
        "#Compute the TP, TN, FP, FN colulns for each model's dataframe\n",
        "for model_index, model_name in models.items():\n",
        "    helper_metric(filepath=f\"analysis/future_data/temporary_2023_2024/es_future_final_{model_name}_2023_2024.csv\", model=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper functions to plot heatmaps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This helper function creates an extended heatmap with averages for rows and columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_extended_heatmap_data(base_data, tp_data, fp_data):\n",
        "    \"\"\"\n",
        "    Return an extended array that appends:\n",
        "      - A new last column with each row's ratio-of-sums\n",
        "      - A new last row with each column's ratio-of-sums\n",
        "      - The bottom-right cell with the ratio-of-total-sums\n",
        "    base_data: the cell-level precision\n",
        "    tp_data, fp_data: the sums of TPs and FPs for each cell\n",
        "    \"\"\"\n",
        "    n_rows, n_cols = base_data.shape\n",
        "\n",
        "    # Prepare the extended array\n",
        "    extended = np.zeros((n_rows + 1, n_cols + 1), dtype=float)\n",
        "\n",
        "    # Fill in the main area\n",
        "    extended[:n_rows, :n_cols] = base_data\n",
        "\n",
        "    # Compute row-average via ratio of sums\n",
        "    for r in range(n_rows):\n",
        "        row_tp = np.sum(tp_data[r, :])\n",
        "        row_fp = np.sum(fp_data[r, :])\n",
        "        if row_tp + row_fp > 0:\n",
        "            extended[r, -1] = row_tp / (row_tp + row_fp)\n",
        "        else:\n",
        "            extended[r, -1] = 0.0\n",
        "\n",
        "    # Compute column-average via ratio of sums\n",
        "    for c in range(n_cols):\n",
        "        col_tp = np.sum(tp_data[:, c])\n",
        "        col_fp = np.sum(fp_data[:, c])\n",
        "        if col_tp + col_fp > 0:\n",
        "            extended[-1, c] = col_tp / (col_tp + col_fp)\n",
        "        else:\n",
        "            extended[-1, c] = 0.0\n",
        "\n",
        "    # Bottom-right cell: ratio of total TPs / total (TPs+FPs)\n",
        "    total_tp = np.sum(tp_data)\n",
        "    total_fp = np.sum(fp_data)\n",
        "    if total_tp + total_fp > 0:\n",
        "        extended[-1, -1] = total_tp / (total_tp + total_fp)\n",
        "    else:\n",
        "        extended[-1, -1] = 0.0\n",
        "\n",
        "    return extended"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def plot_extended_heatmap(extended_data, x_labels, y_labels, xlabel, ylabel, title,\n",
        "                          cmap=plt.cm.RdYlGn, norm_range=(0.65, 0.85), figsize=(14, 10)):\n",
        "    \"\"\"\n",
        "    Plot the heatmap given the extended data and labels.\n",
        "    The x_labels and y_labels should include the label for the extra (average) column/row.\n",
        "    \"\"\"\n",
        "\n",
        "    # Append 'Avg' labels for extra column and row\n",
        "    x_labels = x_labels + ['Avg']\n",
        "    y_labels = y_labels + ['Avg']\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "    norm = plt.Normalize(*norm_range)\n",
        "    heatmap = plt.imshow(extended_data, cmap=cmap, norm=norm, aspect='auto')\n",
        "\n",
        "    # Colorbar with label\n",
        "    cbar = plt.colorbar(heatmap)\n",
        "    cbar.set_label('Mean Precision', fontsize=12)\n",
        "\n",
        "    # Adjust tick sizes\n",
        "    plt.xticks(ticks=np.arange(len(x_labels)), labels=x_labels, fontsize=12, rotation=45, ha='right')\n",
        "    plt.yticks(ticks=np.arange(len(y_labels)), labels=y_labels, fontsize=12)\n",
        "\n",
        "    plt.xlabel(xlabel, fontsize=14)\n",
        "    plt.ylabel(ylabel, fontsize=14)\n",
        "    plt.title(title, fontsize=16)\n",
        "\n",
        "    # Annotate each cell with its numeric value\n",
        "    for i in range(extended_data.shape[0]):\n",
        "        for j in range(extended_data.shape[1]):\n",
        "            value = extended_data[i, j]\n",
        "            plt.text(j, i, f\"{value:.2f}\", ha='center', va='center', color='black', fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    output_path = f\"analysis/heatmaps/temporary_heatmaps/{title}.png\"\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Overwrite with a compressed version\n",
        "    Image.open(output_path).save(output_path, format=\"PNG\", optimize=True, compress_level=9)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def plot_extended_heatmap_volume(extended_data, x_labels, y_labels, xlabel, ylabel, title,\n",
        "                          cmap=plt.cm.viridis, figsize=(14, 10)):\n",
        "    \"\"\"\n",
        "    Plots a heatmap from the provided extended data, displaying row and column averages. \n",
        "    The heatmap is dynamically normalized to improve visual clarity and includes annotated \n",
        "    cell values.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    extended_data : array-like\n",
        "        The data matrix that includes original values plus appended row and column averages.\n",
        "    x_labels : list\n",
        "        Labels for the x-axis corresponding to the columns of the original data.\n",
        "    y_labels : list\n",
        "        Labels for the y-axis corresponding to the rows of the original data.\n",
        "    xlabel : str\n",
        "        Label for the x-axis.\n",
        "    ylabel : str\n",
        "        Label for the y-axis.\n",
        "    title : str\n",
        "        Title of the heatmap.\n",
        "    cmap : matplotlib colormap, optional\n",
        "        Colormap for the heatmap (default is `plt.cm.viridis`).\n",
        "    figsize : tuple, optional\n",
        "        Figure size for the heatmap plot (default is `(14, 10)`).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    None\n",
        "        The function saves the heatmap as a compressed PNG file and displays the plot.\"\n",
        "    \"\"\"\n",
        "\n",
        "    # Append 'Avg' labels for extra column and row\n",
        "    x_labels = x_labels + ['Avg']\n",
        "    y_labels = y_labels + ['Avg']\n",
        "\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Dynamic normalization based on data distribution\n",
        "    lower_bound = np.percentile(extended_data, 5)\n",
        "    upper_bound = np.percentile(extended_data, 95)\n",
        "    norm = plt.Normalize(lower_bound, upper_bound)\n",
        "    heatmap = plt.imshow(extended_data, cmap=cmap, norm=norm, aspect='auto')\n",
        "\n",
        "    # Colorbar with label\n",
        "    cbar = plt.colorbar(heatmap)\n",
        "    cbar.set_label('Number of data points', fontsize=12)\n",
        "\n",
        "    # Adjust tick sizes\n",
        "    plt.xticks(ticks=np.arange(len(x_labels)), labels=x_labels, fontsize=12, rotation=45, ha='right')\n",
        "    plt.yticks(ticks=np.arange(len(y_labels)), labels=y_labels, fontsize=12)\n",
        "\n",
        "    plt.xlabel(xlabel, fontsize=14)\n",
        "    plt.ylabel(ylabel, fontsize=14)\n",
        "    plt.title(title, fontsize=16)\n",
        "\n",
        "    # Annotate each cell with its numeric value\n",
        "    for i in range(extended_data.shape[0]):\n",
        "        for j in range(extended_data.shape[1]):\n",
        "            value = extended_data[i, j]\n",
        "            text_color = 'white' if norm(value) < 0.2 else 'black'\n",
        "            plt.text(j, i, f\"{int(value)}\", ha='center', va='center', color=text_color, fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # Save heatmap image\n",
        "    output_path = f\"analysis/heatmaps/temporary_heatmaps_volume/{title}.png\"\n",
        "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Overwrite with a compressed version\n",
        "    Image.open(output_path).save(output_path, format=\"PNG\", optimize=True, compress_level=9)\n",
        "\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def number_of_data_points(tp_data,fp_data, tn_data, fn_data):\n",
        "    \"\"\"\n",
        "    Calculates the total number of data points by summing true positive (tp_data), \n",
        "    false positive (fp_data), true negative (tn_data), and false negative (fn_data) data values. \n",
        "    Additionally, it appends row averages, column averages, and the overall average to the result.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    tp_data : array-like\n",
        "        Data representing true positive values.\n",
        "    fp_data : array-like\n",
        "        Data representing false positive values.\n",
        "    tn_data : array-like\n",
        "        Data representing true negative values.\n",
        "    fn_data : array-like\n",
        "        Data representing false negative values.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    array-like\n",
        "        A matrix that extends the original `sum_data` by adding:\n",
        "        - Column averages as an additional bottom row\n",
        "        - Row averages as an additional column\n",
        "        - The overall average in the bottom-right corner\n",
        "    \"\"\"\n",
        "\n",
        "    sum_data = tp_data + fp_data + tn_data + fn_data\n",
        "    # Add row and column averages\n",
        "    row_avg = np.mean(sum_data, axis=1).reshape(-1, 1)  # Row averages as column vector\n",
        "    col_avg = np.mean(sum_data, axis=0).reshape(1, -1)  # Column averages as row vector\n",
        "    overall_avg = np.mean(sum_data)                     # Overall average\n",
        "\n",
        "    # Expanded matrix with averages\n",
        "    sum_data_expanded = np.vstack([sum_data, col_avg]) # Add column averages at bottom\n",
        "    sum_data_expanded = np.hstack([sum_data_expanded, np.append(row_avg, overall_avg).reshape(-1, 1)]) # Add row averages + overall avg\n",
        "    return sum_data_expanded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precision computation methods (for heatmaps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "from typing import List, Dict\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_heatmaps(x_values: Dict[int, str], y_values: Dict[int, str], horizons: List[int]) -> Dict[str, Dict[str, List[int]]]:\n",
        "\n",
        "    \"\"\"Initialize heatmaps dictionary to store TP and FP values.\"\"\"\n",
        "    return {\n",
        "        f\"{x_index}_{y_index}\": {\"TP\": [0] * len(horizons), \"FP\": [0] * len(horizons), \"TN\": [0] * len(horizons), \"FN\": [0] * len(horizons)}\n",
        "        for x_index, _ in enumerate(x_values) for y_index, _ in enumerate(y_values)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_data_matrices(len_x_values: int, len_y_values: int):\n",
        "    \"\"\"Initialize matrices to store TP, FP, and precision values.\"\"\"\n",
        "    return (\n",
        "        np.zeros((len_y_values, len_x_values), dtype=float),  # tp_data\n",
        "        np.zeros((len_y_values, len_x_values), dtype=float),  # fp_data\n",
        "        np.zeros((len_y_values, len_x_values), dtype=float),  # precisions\n",
        "        np.zeros((len_y_values, len_x_values), dtype=float),  # tn_data\n",
        "        np.zeros((len_y_values, len_x_values), dtype=float),  # fn_data\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_data(model: str) -> pd.DataFrame:\n",
        "    \"\"\"Load CSV file for a given model.\"\"\"\n",
        "    return pd.read_csv(f\"analysis/future_data/temporary_2023_2024/es_future_final_{model}_2023_2024.csv\", parse_dates=True, index_col=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def update_matrices(heatmaps, tp_data, fp_data, tn_data, fn_data, x_values, y_values):\n",
        "    \"\"\"Update TP and FP matrices based on heatmaps data.\"\"\"\n",
        "    for x_index, _ in enumerate(x_values):\n",
        "        for y_index, _ in enumerate(y_values):\n",
        "            key = f\"{x_index}_{y_index}\"\n",
        "            tp_data[y_index, x_index] = sum(heatmaps[key][\"TP\"])\n",
        "            fp_data[y_index, x_index] = sum(heatmaps[key][\"FP\"])\n",
        "            tn_data[y_index, x_index] = sum(heatmaps[key][\"TN\"])\n",
        "            fn_data[y_index, x_index] = sum(heatmaps[key][\"FN\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fill_precisions(precisions, tp_data, fp_data):\n",
        "    \"\"\"\n",
        "    Fill precisions with precision = TP / (TP + FP).\n",
        "    This is a helper function to encapsulate the logic of computing precisions.\n",
        "    \"\"\"\n",
        "    len_x_values, len_y_values = precisions.shape\n",
        "    for x in range(len_x_values):\n",
        "        for y in range(len_y_values):\n",
        "            total = tp_data[x, y] + fp_data[x, y]\n",
        "            if total > 0:\n",
        "                precisions[x, y] = tp_data[x, y] / total\n",
        "            else:\n",
        "                precisions[x, y] = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_hour_index(extracted_hour_label: str, day_hours_bool: bool, night_hours_bool: bool, all_hours_bool: bool) -> int:\n",
        "    \"\"\"\n",
        "    Find the index corresponding to the extracted hour label in the constant dictionaries defined above.\n",
        "\n",
        "    This function determines the index of a given hour label in one of three possible\n",
        "    dictionaries (`day_hours`, `night_hours`, or `hours`), depending on the provided\n",
        "    boolean flags.\n",
        "\n",
        "    Parameters:\n",
        "    - extracted_hour_label (str): The formatted hour string (e.g., \"3PM\").\n",
        "    - day_hours_bool (bool): Whether to look for the hour in `day_hours`.\n",
        "    - night_hours_bool (bool): Whether to look for the hour in `night_hours`.\n",
        "    - all_hours_bool (bool): Whether to look for the hour in the full `hours` dictionary.\n",
        "\n",
        "    Returns:\n",
        "    - int: The index of the hour in the selected dictionary, or -1 if not found.\n",
        "\n",
        "    Notes:\n",
        "    - The function prioritizes `day_hours`, then `night_hours`, and finally `all_hours`.\n",
        "    - If multiple flags are `True`, only the first matching condition applies.\n",
        "    \"\"\"\n",
        "    # Retrieve the index in day_hours, night_hours, or hours dictionaries.\n",
        "    hour_index = -1\n",
        "    if day_hours_bool and (extracted_hour_label in day_hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in day_hours.items() if hour_label == extracted_hour_label)\n",
        "    elif night_hours_bool and (extracted_hour_label in night_hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in night_hours.items() if hour_label == extracted_hour_label)\n",
        "    elif all_hours_bool and (extracted_hour_label in hours.values()):\n",
        "        hour_index = next(hour_index for hour_index, hour_label in hours.items() if hour_label == extracted_hour_label)\n",
        "    return hour_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def precision(models: List[str], x_values, y_values, x_labels, y_labels, xlabel, ylabel, title, day_hours_bool: bool, night_hours_bool: bool, all_hours_bool: bool, process_row, **kwargs):\n",
        "    \"\"\"Compute precision matrices for each model, fill precisions, and plot extended heatmap.\"\"\"\n",
        "\n",
        "    # 1. Initialize data structures\n",
        "    heatmaps = initialize_heatmaps(x_values, y_values, horizons)\n",
        "    tp_data, fp_data, precisions, tn_data, fn_data = initialize_data_matrices(len(x_values), len(y_values))\n",
        "\n",
        "    # 2. Collect TP and FP values from each model\n",
        "    for model_index, model in models.items():\n",
        "        df = load_model_data(model)\n",
        "        \n",
        "        # The context has a length of 384 hours and so ends at index context_length_index. We start predicting at the 385th hour (at index 384). \n",
        "        # The last prediction_length hours of the datframe are not considered for prediction because they are used for comparison with the last 12 predictions (for the last hour of the set). \n",
        "        # We predict the next prediction_length hours for every row.\n",
        "        first_predicted_row, last_predicted_row = context_length_index, len(df) - prediction_length\n",
        "\n",
        "        for index, row in df[first_predicted_row:last_predicted_row].iterrows():\n",
        "            kwargs[\"index\"], kwargs[\"model_index\"], kwargs[\"row\"], kwargs[\"heatmaps\"],kwargs[\"day_hours_bool\"], kwargs[\"night_hours_bool\"], kwargs[\"all_hours_bool\"] = index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool\n",
        "            process_row(**kwargs)\n",
        "\n",
        "    # 3. Aggregate results into tp_data and fp_data\n",
        "    update_matrices(heatmaps=heatmaps, tp_data=tp_data, fp_data=fp_data, tn_data=tn_data, fn_data=fn_data, x_values= x_values, y_values=y_values)\n",
        "\n",
        "    # 4. Fill precisions with the ratio = TP / (TP + FP)\n",
        "    fill_precisions(precisions, tp_data, fp_data)\n",
        "\n",
        "    # 5. Build the volume heatmap and plot\n",
        "    extended_data_sum = number_of_data_points(tp_data=tp_data, fp_data=fp_data, tn_data=tn_data, fn_data=fn_data)\n",
        "    plot_extended_heatmap_volume(\n",
        "        extended_data_sum, \n",
        "        x_labels=x_labels,\n",
        "        y_labels=y_labels,\n",
        "        xlabel=xlabel,\n",
        "        ylabel=ylabel,\n",
        "        title=f\"Heatmap by Volume ({xlabel} vs {ylabel})\" + (\" - Day Hours\" if day_hours_bool else \"\") + (\" - Night Hours\" if night_hours_bool else \"\")\n",
        "    )\n",
        "\n",
        "    # 6. Build extended heatmap data & plot\n",
        "    extended_data = create_extended_heatmap_data(precisions, tp_data, fp_data)\n",
        "    plot_extended_heatmap(\n",
        "        extended_data, \n",
        "        x_labels=x_labels,\n",
        "        y_labels=y_labels,\n",
        "        xlabel=xlabel,\n",
        "        ylabel=ylabel,\n",
        "        title=title\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Days x Hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_days_hours(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for day and hour settings.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "    \n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day = index.weekday()\n",
        "\n",
        "    # Convert hour into a readable format (e.g., \"3PM\" instead of 15)\n",
        "    extracted_hour_label = datetime.strptime(str(index.hour), \"%H\").strftime(\"%-I%p\")\n",
        "\n",
        "    # Determine the corresponding index of an hour in the dictionary storing all constants\n",
        "    hour_index = find_hour_index(extracted_hour_label, day_hours_bool, night_hours_bool, all_hours_bool)\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if (day in days.keys()):\n",
        "\n",
        "        # Apply filtering logic based on trading hours settings (day hours, night hours, or all hours)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())):\n",
        "            \n",
        "            # Convert TP and FP values from string representations to lists\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            TN_list = ast.literal_eval(row[\"TN\"])\n",
        "            FN_list = ast.literal_eval(row[\"FN\"])\n",
        "\n",
        "            # Iterate over different forecast horizons and update heatmap values\n",
        "            for horizon_index in horizons:\n",
        "                    heatmaps[f\"{day}_{hour_index}\"][\"TP\"][horizon_index] += TP_list[horizon_index]\n",
        "                    heatmaps[f\"{day}_{hour_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "                    heatmaps[f\"{day}_{hour_index}\"][\"TN\"][horizon_index] += TN_list[horizon_index]\n",
        "                    heatmaps[f\"{day}_{hour_index}\"][\"FN\"][horizon_index] += FN_list[horizon_index]\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=days, y_values=hours, x_labels = day_labels, y_labels = hour_labels, xlabel=\"Day of the Week\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Days x Hours)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row_days_hours, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=day_hours, x_labels = day_labels, y_labels = day_hours_labels, xlabel=\"Day of the Week\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Days x Hours) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row_days_hours, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=night_hours, x_labels = day_labels, y_labels = night_hours_labels, xlabel=\"Day of the Week\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Days x Hours) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row_days_hours, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Models x Hours)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_models_hours(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for model and hour settings.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "\n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day = index.weekday()\n",
        "\n",
        "    # Convert hour into a readable format (e.g., \"3PM\" instead of 15)\n",
        "    extracted_hour_label = datetime.strptime(str(index.hour), \"%H\").strftime(\"%-I%p\")\n",
        "\n",
        "    # Determine the corresponding index of an hour in the dictionary storing all constants\n",
        "    hour_index = find_hour_index(extracted_hour_label, day_hours_bool, night_hours_bool, all_hours_bool)\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if day in days.keys():\n",
        "        \n",
        "        # Apply filtering logic based on trading hours settings (day hours, night hours, or all hours)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())):\n",
        "            \n",
        "            # Convert TP and FP values from string representations to lists\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            TN_list = ast.literal_eval(row[\"TN\"])\n",
        "            FN_list = ast.literal_eval(row[\"FN\"])\n",
        "            \n",
        "            # Iterate over different forecast horizons and update heatmap values\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{model_index}_{hour_index}\"][\"TP\"][horizon_index] += TP_list[horizon_index]\n",
        "                heatmaps[f\"{model_index}_{hour_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "                heatmaps[f\"{model_index}_{hour_index}\"][\"TN\"][horizon_index] += TN_list[horizon_index]\n",
        "                heatmaps[f\"{model_index}_{hour_index}\"][\"FN\"][horizon_index] += FN_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=models, y_values=hours, x_labels = models_label, y_labels = hour_labels, xlabel=\"Models\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Models x Hours)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row_models_hours, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=day_hours, x_labels = models_label, y_labels = day_hours_labels, xlabel=\"Models\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Models x Hours) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row_models_hours, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=night_hours, x_labels = models_label, y_labels = night_hours_labels, xlabel=\"Models\", ylabel=\"Entry Hour\", title=\"Mean Precision Heatmap (Models x Hours) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row_models_hours, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Models x Horizon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_model_horizon(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for model and horizon settings.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "\n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day = index.weekday()\n",
        "    \n",
        "     # Convert hour into a readable format (e.g., \"3PM\" instead of 15)\n",
        "    extracted_hour_label = datetime.strptime(str(index.hour), \"%H\").strftime(\"%-I%p\")\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if (day in days.keys()):\n",
        "        \n",
        "        # Apply filtering logic based on trading hours settings (day hours, night hours, or all hours)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())):\n",
        "            \n",
        "            # Convert TP and FP values from string representations to lists\n",
        "            TP_List = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            TN_list = ast.literal_eval(row[\"TN\"])\n",
        "            FN_list = ast.literal_eval(row[\"FN\"])\n",
        "            \n",
        "            # Iterate over different forecast horizons and update heatmap values\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{model_index}_{horizon_index }\"][\"TP\"][horizon_index] += TP_List[horizon_index]\n",
        "                heatmaps[f\"{model_index}_{horizon_index }\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "                heatmaps[f\"{model_index}_{horizon_index }\"][\"TN\"][horizon_index] += TN_list[horizon_index]\n",
        "                heatmaps[f\"{model_index}_{horizon_index }\"][\"FN\"][horizon_index] += FN_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=models, y_values=horizons, x_labels = models_label, y_labels = horizons_label, xlabel=\"Models\", ylabel=\"Horizons\", title=\"Mean Precision Heatmap (Models x Horizons)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row_model_horizon, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=horizons, x_labels = models_label, y_labels = horizons_label, xlabel=\"Models\", ylabel=\"Horizons\", title=\"Mean Precision Heatmap (Models x Horizons) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row_model_horizon, **kwargs)\n",
        "precision(models=models, x_values=models, y_values=horizons, x_labels = models_label, y_labels = horizons_label, xlabel=\"Models\", ylabel=\"Horizons\", title=\"Mean Precision Heatmap (Models x Horizons) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row_model_horizon, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Days x Models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row(**kwargs):\n",
        "    \"\"\"Process a single row, updating heatmaps with TP and FP values.\"\"\"\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "    hour_index = index.hour\n",
        "    extracted_hour_label = datetime.strptime(str(hour_index), \"%H\").strftime(\"%-I%p\")\n",
        "    day_index = index.weekday()\n",
        "\n",
        "    if (day_index in days.keys()): # Skip Saturday and Sunday because the market is closed (we applied a forward fill to fill the missing values but we don't want to evaluate them)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())): # Filter for day or night hours (if triggered)\n",
        "            TP_List = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            TN_list = ast.literal_eval(row[\"TN\"])\n",
        "            FN_list = ast.literal_eval(row[\"FN\"])\n",
        "\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"TP\"][horizon_index] += TP_List[horizon_index]\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"TN\"][horizon_index] += TN_list[horizon_index]\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"FN\"][horizon_index] += FN_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_days_models(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for day and model settings.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    \"\"\"\n",
        "\n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "\n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day_index = index.weekday()\n",
        "\n",
        "    # Extract the hour directly\n",
        "    hour_index = index.hour\n",
        "\n",
        "    # Convert hour into a readable format (e.g., \"3PM\" instead of 15)\n",
        "    extracted_hour_label = datetime.strptime(str(hour_index), \"%H\").strftime(\"%-I%p\")\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if day_index in days.keys():\n",
        "\n",
        "        # Apply filtering logic based on trading hours settings (day hours, night hours, or all hours)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())):\n",
        "            \n",
        "            # Convert TP and FP values from string representations to lists\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            TN_list = ast.literal_eval(row[\"TN\"])\n",
        "            FN_list = ast.literal_eval(row[\"FN\"])\n",
        "\n",
        "            # Iterate over different forecast horizons and update heatmap values\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"TP\"][horizon_index] += TP_list[horizon_index]\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"TN\"][horizon_index] += TN_list[horizon_index]\n",
        "                heatmaps[f\"{day_index}_{model_index}\"][\"FN\"][horizon_index] += FN_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row_days_models, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row_days_models, **kwargs)\n",
        "precision(models=models, x_values=days, y_values=models, x_labels = day_labels, y_labels = models_label, xlabel=\"Days\", ylabel=\"Models\", title=\"Mean Precision Heatmap (Days x Models) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row_days_models, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Heatmap (Quarters x Months)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_quarters_models(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for quarters and models.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    - The data is grouped by quarter based on the month index.\n",
        "    - Months are categorized into quarters as follows:\n",
        "        - Q1: Jan (0), Feb (1), Mar (2)\n",
        "        - Q2: Apr (3), May (4), Jun (5)\n",
        "        - Q3: Jul (6), Aug (7), Sep (8)\n",
        "        - Q4: Oct (9), Nov (10), Dec (11)\n",
        "    \"\"\"\n",
        "\n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "\n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day_index = index.weekday()\n",
        "\n",
        "    # Extract the month (1=January, 12=December) and determine the corresponding quarter (0=Q1, 3=Q4)\n",
        "    month_index = index.month\n",
        "    quarter_index = (month_index - 1) // 3\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if day_index in days.keys():\n",
        "        \n",
        "        # Convert TP and FP values from string representations to lists\n",
        "        TP_list = ast.literal_eval(row[\"TP\"])\n",
        "        FP_list = ast.literal_eval(row[\"FP\"])\n",
        "        TN_list = ast.literal_eval(row[\"TN\"])\n",
        "        FN_list = ast.literal_eval(row[\"FN\"])\n",
        "\n",
        "        # Iterate over different forecast horizons and update heatmap values\n",
        "        for horizon_index in range(len(horizons)):\n",
        "            heatmaps[f\"{quarter_index}_{model_index}\"][\"TP\"][horizon_index] += TP_list[horizon_index]\n",
        "            heatmaps[f\"{quarter_index}_{model_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "            heatmaps[f\"{quarter_index}_{model_index}\"][\"TN\"][horizon_index] += TN_list[horizon_index]\n",
        "            heatmaps[f\"{quarter_index}_{model_index}\"][\"FN\"][horizon_index] += FN_list[horizon_index]\n",
        "\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=quarters, y_values={0: \"Month 1\", 1: \"Month 2\", 2: \"Month 3\"}, x_labels = quarters_label, y_labels = [\"Month 1\", \"Month 2\", \"Month 3\"], xlabel=\"Quarters\", ylabel=\"Months\", title=\"Mean Precision Heatmap (Quarters x Months)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row_quarters_models, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Mean Precision Heatmaps (Quarters x Weeks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_row_quarters_weeks(**kwargs):\n",
        "    \"\"\"\n",
        "    Process a single row of data to update the heatmaps with True Positive (TP) \n",
        "    and False Positive (FP) values using a filtering logic for quarters and weeks.\n",
        "\n",
        "    This function extracts relevant time-based indices from the provided row \n",
        "    and updates the corresponding heatmap values for different forecasting horizons.\n",
        "\n",
        "    Parameters:\n",
        "    - kwargs: Dictionary containing:\n",
        "        - index (datetime): Timestamp index for the row.\n",
        "        - model_index (int): Index referring to the model being processed.\n",
        "        - row (pd.Series): Row containing TP and FP values as stringified lists.\n",
        "        - heatmaps (dict): Dictionary storing heatmap data.\n",
        "        - day_hours_bool (bool): Whether to consider only day trading hours.\n",
        "        - night_hours_bool (bool): Whether to consider only night trading hours.\n",
        "        - all_hours_bool (bool): Whether to consider all trading hours.\n",
        "\n",
        "    Notes:\n",
        "    - The function skips weekends (Saturday and Sunday) as markets are closed.\n",
        "    - Data is grouped into quarters and weeks:\n",
        "        - Weeks are indexed from 0 to 12 within each quarter.\n",
        "        - Quarters are determined based on the ISO calendar week number.\n",
        "    - The filtering logic ensures that only relevant trading hours are considered.\n",
        "    \"\"\"\n",
        "\n",
        "    # Unpack keyword arguments\n",
        "    index, model_index, row, heatmaps, day_hours_bool, night_hours_bool, all_hours_bool = kwargs.values()\n",
        "\n",
        "    # Extract the day of the week (0=Monday, 6=Sunday)\n",
        "    day_index = index.weekday()\n",
        "\n",
        "    # Extract the hour directly\n",
        "    hour_index = index.hour\n",
        "\n",
        "    # Convert hour into a readable format (e.g., \"3PM\" instead of 15)\n",
        "    extracted_hour_label = datetime.strptime(str(hour_index), \"%H\").strftime(\"%-I%p\")\n",
        "\n",
        "    # Extract the ISO week number (1-52) and adjust to zero-based indexing\n",
        "    week_index = index.isocalendar()[1] - 1\n",
        "\n",
        "    # Determine the corresponding quarter and week within that quarter\n",
        "    quarter_index = week_index // 13  # Each quarter has 13 weeks\n",
        "    week_index = week_index % 13  # Keep week index within the quarter range\n",
        "\n",
        "    # Only process rows for trading days (Monday to Friday)\n",
        "    # Market data is forward-filled over weekends, but we don't evaluate those periods\n",
        "    if day_index in days.keys():\n",
        "\n",
        "        # Apply filtering logic based on trading hours settings (day hours, night hours, or all hours)\n",
        "        if all_hours_bool or (day_hours_bool and (extracted_hour_label in day_hours.values())) or (night_hours_bool and (extracted_hour_label in night_hours.values())):\n",
        "            \n",
        "            # Convert TP and FP values from string representations to lists\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            TN_list = ast.literal_eval(row[\"TN\"])\n",
        "            FN_list = ast.literal_eval(row[\"FN\"])\n",
        "\n",
        "            # Iterate over different forecast horizons and update heatmap values\n",
        "            for horizon_index in range(len(horizons)):\n",
        "                heatmaps[f\"{quarter_index}_{week_index}\"][\"TP\"][horizon_index] += TP_list[horizon_index]\n",
        "                heatmaps[f\"{quarter_index}_{week_index}\"][\"FP\"][horizon_index] += FP_list[horizon_index]\n",
        "                heatmaps[f\"{quarter_index}_{week_index}\"][\"TN\"][horizon_index] += TN_list[horizon_index]\n",
        "                heatmaps[f\"{quarter_index}_{week_index}\"][\"FN\"][horizon_index] += FN_list[horizon_index]\n",
        "\n",
        "kwargs = {}\n",
        "precision(models=models, x_values=quarters, y_values=weeks, x_labels = quarters_label, y_labels = weeks_label, xlabel=\"Quarters\", ylabel=\"Weeks\", title=\"2024_2025_Mean Precision Heatmap (Quarters x Weeks)\", day_hours_bool=False, night_hours_bool=False, all_hours_bool=True, process_row=process_row_quarters_weeks, **kwargs)\n",
        "precision(models=models, x_values=quarters, y_values=weeks, x_labels = quarters_label, y_labels = weeks_label, xlabel=\"Quarters\", ylabel=\"Weeks\", title=\"Mean Precision Heatmap (Quarters x Weeks) - day hours\", day_hours_bool=True, night_hours_bool=False, all_hours_bool=False, process_row=process_row_quarters_weeks, **kwargs)\n",
        "precision(models=models, x_values=quarters, y_values=weeks, x_labels = quarters_label, y_labels = weeks_label, xlabel=\"Quarters\", ylabel=\"Weeks\", title=\"Mean Precision Heatmap (Quarters x Weeks) - night hours\", day_hours_bool=False, night_hours_bool=True, all_hours_bool=False, process_row=process_row_quarters_weeks, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix (Good/Bad), (Moirai, Time-MoE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import ast\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "# Ensure the output directory exists\n",
        "output_dir = \"analysis/confusion_matrix/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def get_model_metrics(horizon_idx):\n",
        "    # 4 compteurs : (bon/mauvais Moirai) x (bon/mauvais Time-MoE)\n",
        "    good_moirai_good_moe = 0\n",
        "    good_moirai_bad_moe  = 0\n",
        "    bad_moirai_good_moe  = 0\n",
        "    bad_moirai_bad_moe   = 0\n",
        "\n",
        "    # Load models\n",
        "    df_moirai   = load_model_data(\"moirai\")\n",
        "    df_time_moe = load_model_data(\"time_moe\")\n",
        "\n",
        "    # Vérifie qu'on peut les parcourir en parallèle\n",
        "    assert len(df_moirai) == len(df_time_moe), \"Les deux DataFrames doivent avoir la même taille.\"\n",
        "\n",
        "    first_predicted_row, last_predicted_row = context_length_index, len(df_moirai) - prediction_length\n",
        "\n",
        "    moirai_rows   = df_moirai[first_predicted_row:last_predicted_row].iterrows()\n",
        "    time_moe_rows = df_time_moe[first_predicted_row:last_predicted_row].iterrows()\n",
        "\n",
        "    for (index_moirai, row_moirai), (index_time_moe, row_time_moe) in zip(moirai_rows, time_moe_rows):\n",
        "        day_index = index_moirai.weekday()\n",
        "        if day_index not in days.keys():\n",
        "            continue\n",
        "\n",
        "        # Listes TP/FP/TN/FN de Moirai\n",
        "        moirai_tp_list = ast.literal_eval(row_moirai[\"TP\"])\n",
        "        moirai_fp_list = ast.literal_eval(row_moirai[\"FP\"])\n",
        "        moirai_tn_list = ast.literal_eval(row_moirai[\"TN\"])\n",
        "        moirai_fn_list = ast.literal_eval(row_moirai[\"FN\"])\n",
        "\n",
        "        # Listes TP/FP/TN/FN de Time-MoE\n",
        "        time_moe_tp_list = ast.literal_eval(row_time_moe[\"TP\"])\n",
        "        time_moe_fp_list = ast.literal_eval(row_time_moe[\"FP\"])\n",
        "        time_moe_tn_list = ast.literal_eval(row_time_moe[\"TN\"])\n",
        "        time_moe_fn_list = ast.literal_eval(row_time_moe[\"FN\"])\n",
        "\n",
        "        # On récupère la valeur pour l'horizon donné\n",
        "        moirai_tp = moirai_tp_list[horizon_idx]\n",
        "        moirai_fp = moirai_fp_list[horizon_idx]\n",
        "        moirai_tn = moirai_tn_list[horizon_idx]\n",
        "        moirai_fn = moirai_fn_list[horizon_idx]\n",
        "\n",
        "        time_moe_tp = time_moe_tp_list[horizon_idx]\n",
        "        time_moe_fp = time_moe_fp_list[horizon_idx]\n",
        "        time_moe_tn = time_moe_tn_list[horizon_idx]\n",
        "        time_moe_fn = time_moe_fn_list[horizon_idx]\n",
        "\n",
        "        # Détermine si Moirai est \"bon\" ou \"mauvais\"\n",
        "        moirai_good = (moirai_tp > 0 or moirai_tn > 0)\n",
        "        # Détermine si Time-MoE est \"bon\" ou \"mauvais\"\n",
        "        time_moe_good = (time_moe_tp > 0 or time_moe_tn > 0)\n",
        "\n",
        "        # Incrémente la case correspondant à la combinaison\n",
        "        if moirai_good and time_moe_good:\n",
        "            good_moirai_good_moe += 1\n",
        "        elif moirai_good and not time_moe_good:\n",
        "            good_moirai_bad_moe += 1\n",
        "        elif not moirai_good and time_moe_good:\n",
        "            bad_moirai_good_moe += 1\n",
        "        else:\n",
        "            bad_moirai_bad_moe += 1\n",
        "\n",
        "    # On calcule la matrice\n",
        "    total = good_moirai_good_moe + good_moirai_bad_moe + bad_moirai_good_moe + bad_moirai_bad_moe\n",
        "    if total == 0:\n",
        "        # Eviter la division par zéro, on peut renvoyer une matrice nulle ou vide\n",
        "        return np.zeros((2, 2))\n",
        "\n",
        "    conf_matrix = np.array([\n",
        "        [100.0 * good_moirai_good_moe / total, 100.0 * bad_moirai_good_moe / total],\n",
        "        [100.0 * good_moirai_bad_moe  / total, 100.0 * bad_moirai_bad_moe  / total]\n",
        "    ])\n",
        "\n",
        "    return conf_matrix\n",
        "\n",
        "\n",
        "# -- Génération pour 2 horizons différents --\n",
        "conf_matrix_h1 = get_model_metrics(0)  # H=1\n",
        "conf_matrix_h3 = get_model_metrics(2)  # H=3\n",
        "\n",
        "# -- Mise à l'échelle commune --\n",
        "vmin = min(conf_matrix_h1.min(), conf_matrix_h3.min())\n",
        "vmax = max(conf_matrix_h1.max(), conf_matrix_h3.max())\n",
        "\n",
        "# -- Plotting --\n",
        "plt.rcParams.update({'font.size': 14})\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 10))\n",
        "\n",
        "ax = sns.heatmap(conf_matrix_h1, annot=True, fmt='.2f', cmap='Blues', cbar=True,\n",
        "            vmin=vmin, vmax=vmax, ax=axes[0])\n",
        "axes[0].set_title(\"Confusion Matrix Heatmap (H=1)\")\n",
        "axes[0].set_xticks([0.5, 1.5])\n",
        "axes[0].set_xticklabels(['Good Prediction Moirai', 'Bad Prediction Moirai'])\n",
        "axes[0].set_yticks([0.5, 1.5])\n",
        "axes[0].set_yticklabels(['Good Prediction Time-MoE', 'Bad Prediction Time-MoE'])\n",
        "\n",
        "colorbar = ax.collections[0].colorbar\n",
        "colorbar.set_ticks(np.linspace(vmin, vmax, 5))  # 5 evenly spaced ticks within range\n",
        "colorbar.ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x:.0f}%'))\n",
        "\n",
        "\n",
        "ax = sns.heatmap(conf_matrix_h3, annot=True, fmt='.2f', cmap='Blues', cbar=True,\n",
        "            vmin=vmin, vmax=vmax, ax=axes[1])\n",
        "axes[1].set_title(\"Confusion Matrix Heatmap (H=3)\")\n",
        "axes[1].set_xticks([0.5, 1.5])\n",
        "axes[1].set_xticklabels(['Good Prediction Moirai', 'Bad Prediction Moirai'])\n",
        "axes[1].set_yticks([0.5, 1.5])\n",
        "axes[1].set_yticklabels([])  # Optionnel pour l'esthétique\n",
        "\n",
        "colorbar = ax.collections[0].colorbar\n",
        "colorbar.set_ticks(np.linspace(vmin, vmax, 5))\n",
        "colorbar.ax.yaxis.set_major_formatter(mticker.FuncFormatter(lambda x, _: f'{x:.0f}%'))\n",
        "\n",
        "combined_output_path = os.path.join(output_dir, \"Confusion Matrix Moirai Time-Moe.png\")\n",
        "\n",
        "plt.savefig(combined_output_path, dpi=300, bbox_inches='tight')\n",
        "\n",
        "Image.open(combined_output_path).save(combined_output_path, format=\"PNG\", optimize=True, compress_level=9)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import ast\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "# Ensure the output directory exists\n",
        "output_dir = \"analysis/confusion_matrix/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "def get_model_metrics(horizon_idx):\n",
        "\n",
        "    up_moirai_up_moe = {\"count\": 0, \"TP_moirai\": 0, \"TP_moe\": 0, \"FP_moirai\": 0, \"FP_moe\": 0, \"TN_moirai\": 0, \"TN_moe\": 0, \"FN_moirai\": 0, \"FN_moe\": 0}\n",
        "    up_moirai_down_moe = {\"count\": 0, \"TP_moirai\": 0, \"TP_moe\": 0, \"FP_moirai\": 0, \"FP_moe\": 0, \"TN_moirai\": 0, \"TN_moe\": 0, \"FN_moirai\": 0, \"FN_moe\": 0}\n",
        "    down_moirai_up_moe = {\"count\": 0, \"TP_moirai\": 0, \"TP_moe\": 0, \"FP_moirai\": 0, \"FP_moe\": 0, \"TN_moirai\": 0, \"TN_moe\": 0, \"FN_moirai\": 0, \"FN_moe\": 0}\n",
        "    down_moirai_down_moe = {\"count\": 0, \"TP_moirai\": 0, \"TP_moe\": 0, \"FP_moirai\": 0, \"FP_moe\": 0, \"TN_moirai\": 0, \"TN_moe\": 0, \"FN_moirai\": 0, \"FN_moe\": 0}\n",
        "\n",
        "    # Load models\n",
        "    df_moirai   = load_model_data(\"moirai\")\n",
        "    df_time_moe = load_model_data(\"time_moe\")\n",
        "\n",
        "    # Vérifie qu'on peut les parcourir en parallèle\n",
        "    assert len(df_moirai) == len(df_time_moe), \"Les deux DataFrames doivent avoir la même taille.\"\n",
        "\n",
        "    first_predicted_row, last_predicted_row = context_length_index, len(df_moirai) - prediction_length\n",
        "\n",
        "    moirai_rows   = df_moirai[first_predicted_row:last_predicted_row].iterrows()\n",
        "    time_moe_rows = df_time_moe[first_predicted_row:last_predicted_row].iterrows()\n",
        "\n",
        "    for i, ((index_moirai, row_moirai), (index_time_moe, row_time_moe)) in enumerate(zip(moirai_rows, time_moe_rows)):\n",
        "        day_index = index_moirai.weekday()\n",
        "        # ignore the first row\n",
        "        if day_index not in days.keys():\n",
        "            continue\n",
        "\n",
        "        # Listes TP/FP/TN/FN de Moirai\n",
        "        moirai_tp_list, moirai_fp_list = ast.literal_eval(row_moirai[\"TP\"]), ast.literal_eval(row_moirai[\"FP\"])\n",
        "        moirai_tn_list, moirai_fn_list = ast.literal_eval(row_moirai[\"TN\"]), ast.literal_eval(row_moirai[\"FN\"])\n",
        "\n",
        "        # Listes TP/FP/TN/FN de Time-MoE\n",
        "        time_moe_tp_list, time_moe_fp_list= ast.literal_eval(row_time_moe[\"TP\"]), ast.literal_eval(row_time_moe[\"FP\"])\n",
        "        time_moe_tn_list, time_moe_fn_list = ast.literal_eval(row_time_moe[\"TN\"]), ast.literal_eval(row_time_moe[\"FN\"])\n",
        "\n",
        "        # On récupère la valeur pour l'horizon donné\n",
        "        moirai_tp, moirai_fp, moirai_tn, moirai_fn = moirai_tp_list[horizon_idx], moirai_fp_list[horizon_idx], moirai_tn_list[horizon_idx], moirai_fn_list[horizon_idx]\n",
        "        time_moe_tp, time_moe_fp, time_moe_tn, time_moe_fn = time_moe_tp_list[horizon_idx], time_moe_fp_list[horizon_idx], time_moe_tn_list[horizon_idx], time_moe_fn_list[horizon_idx]\n",
        "\n",
        "        print(moirai_tp, moirai_fp, moirai_tn, moirai_fn)\n",
        "        print(time_moe_tp, time_moe_fp, time_moe_tn, time_moe_fn)\n",
        "\n",
        "        current_predicted_value_horizon_moirai = ast.literal_eval(row_moirai[\"Result\"])[horizon_idx]\n",
        "        current_predicted_value_horizon_time_moe = ast.literal_eval(row_time_moe[\"Result\"])[horizon_idx]\n",
        "\n",
        "        moirai_up = False\n",
        "        moirai_down = False\n",
        "        time_moe_up = False\n",
        "        time_moe_down = False\n",
        "\n",
        "        previous_value_horizon_moirai = float(row_moirai[\"Close\"])\n",
        "        previous_value_horizon_time_moe = float(row_time_moe[\"Close\"])\n",
        "\n",
        "        print(previous_value_horizon_moirai, previous_value_horizon_time_moe)\n",
        "\n",
        "        if current_predicted_value_horizon_moirai > previous_value_horizon_moirai:\n",
        "            moirai_up = True\n",
        "        if current_predicted_value_horizon_moirai < previous_value_horizon_moirai:\n",
        "            moirai_down = True\n",
        "        if current_predicted_value_horizon_time_moe > previous_value_horizon_time_moe:\n",
        "            time_moe_up = True\n",
        "        if current_predicted_value_horizon_time_moe < previous_value_horizon_time_moe:\n",
        "            time_moe_down = True\n",
        "        \n",
        "        # fill the values up_moirai_up_moe, etc.\n",
        "        if moirai_up and time_moe_up:\n",
        "            up_moirai_up_moe[\"count\"] += 1\n",
        "            up_moirai_up_moe[\"TP_moirai\"] += moirai_tp\n",
        "            up_moirai_up_moe[\"TP_moe\"] += time_moe_tp\n",
        "            up_moirai_up_moe[\"FP_moirai\"] += moirai_fp\n",
        "            up_moirai_up_moe[\"FP_moe\"] += time_moe_fp\n",
        "            up_moirai_up_moe[\"TN_moirai\"] += moirai_tn\n",
        "            up_moirai_up_moe[\"TN_moe\"] += time_moe_tn\n",
        "            up_moirai_up_moe[\"FN_moirai\"] += moirai_fn\n",
        "            up_moirai_up_moe[\"FN_moe\"] += time_moe_fn\n",
        "        elif moirai_up and time_moe_down:\n",
        "            up_moirai_down_moe[\"count\"] += 1\n",
        "            up_moirai_down_moe[\"TP_moirai\"] += moirai_tp\n",
        "            up_moirai_down_moe[\"TP_moe\"] += time_moe_tp\n",
        "            up_moirai_down_moe[\"FP_moirai\"] += moirai_fp\n",
        "            up_moirai_down_moe[\"FP_moe\"] += time_moe_fp\n",
        "            up_moirai_down_moe[\"TN_moirai\"] += moirai_tn\n",
        "            up_moirai_down_moe[\"TN_moe\"] += time_moe_tn\n",
        "            up_moirai_down_moe[\"FN_moirai\"] += moirai_fn\n",
        "            up_moirai_down_moe[\"FN_moe\"] += time_moe_fn\n",
        "        elif moirai_down and time_moe_up:\n",
        "            down_moirai_up_moe[\"count\"] += 1\n",
        "            down_moirai_up_moe[\"TP_moirai\"] += moirai_tp\n",
        "            down_moirai_up_moe[\"TP_moe\"] += time_moe_tp\n",
        "            down_moirai_up_moe[\"FP_moirai\"] += moirai_fp\n",
        "            down_moirai_up_moe[\"FP_moe\"] += time_moe_fp\n",
        "            down_moirai_up_moe[\"TN_moirai\"] += moirai_tn\n",
        "            down_moirai_up_moe[\"TN_moe\"] += time_moe_tn\n",
        "            down_moirai_up_moe[\"FN_moirai\"] += moirai_fn\n",
        "            down_moirai_up_moe[\"FN_moe\"] += time_moe_fn\n",
        "        elif moirai_down and time_moe_down:\n",
        "            down_moirai_down_moe[\"count\"] += 1\n",
        "            down_moirai_down_moe[\"TP_moirai\"] += moirai_tp\n",
        "            down_moirai_down_moe[\"TP_moe\"] += time_moe_tp\n",
        "            down_moirai_down_moe[\"FP_moirai\"] += moirai_fp\n",
        "            down_moirai_down_moe[\"FP_moe\"] += time_moe_fp\n",
        "            down_moirai_down_moe[\"TN_moirai\"] += moirai_tn\n",
        "            down_moirai_down_moe[\"TN_moe\"] += time_moe_tn\n",
        "            down_moirai_down_moe[\"FN_moirai\"] += moirai_fn\n",
        "            down_moirai_down_moe[\"FN_moe\"] += time_moe_fn\n",
        "        # add everything in the dict\n",
        "\n",
        "    return up_moirai_up_moe, up_moirai_down_moe, down_moirai_up_moe, down_moirai_down_moe\n",
        "\n",
        "\n",
        "up_moirai_up_moe_1, up_moirai_down_moe_1, down_moirai_up_moe_1, down_moirai_down_moe_1 = get_model_metrics(0)  # H=1\n",
        "\n",
        "precision_moirai_up_up = 100.0 * up_moirai_up_moe_1[\"TP_moirai\"] / (up_moirai_up_moe_1[\"TP_moirai\"] + up_moirai_up_moe_1[\"FP_moirai\"])\n",
        "precision_moirai_up_down = 100.0 * up_moirai_down_moe_1[\"TP_moirai\"] / (up_moirai_down_moe_1[\"TP_moirai\"] + up_moirai_down_moe_1[\"FP_moirai\"])\n",
        "precision_moirai_down_up = 100.0 * down_moirai_up_moe_1[\"TP_moirai\"] / (down_moirai_up_moe_1[\"TP_moirai\"] + down_moirai_up_moe_1[\"FP_moirai\"])\n",
        "precision_moirai_down_down = 100.0 * down_moirai_down_moe_1[\"TP_moirai\"] / (down_moirai_down_moe_1[\"TP_moirai\"] + down_moirai_down_moe_1[\"FP_moirai\"])\n",
        "\n",
        "# build a simple confuson matrix for the first horizon and for the first model moirai\n",
        "\n",
        "# Precision matrix\n",
        "precision_matrix = np.array([\n",
        "    [precision_moirai_up_up, precision_moirai_up_down],   # Moirai Up\n",
        "    [precision_moirai_down_up, precision_moirai_down_down] # Moirai Down\n",
        "])\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(precision_matrix, annot=True, fmt=\".2f\", cmap='Blues',\n",
        "            xticklabels=['Time-Moe Up', 'Time-Moe Down'],\n",
        "            yticklabels=['Moirai Up', 'Moirai Down'])\n",
        "plt.title('Precision for Moirai (H=1)')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "precision_moe_up_up = 100.0 * up_moirai_up_moe_1[\"TP_moe\"] / (up_moirai_up_moe_1[\"TP_moe\"] + up_moirai_up_moe_1[\"FP_moe\"])\n",
        "precision_moe_up_down = 100.0 * up_moirai_down_moe_1[\"TP_moe\"] / (up_moirai_down_moe_1[\"TP_moe\"] + up_moirai_down_moe_1[\"FP_moe\"])\n",
        "precision_moe_down_up = 100.0 * down_moirai_up_moe_1[\"TP_moe\"] / (down_moirai_up_moe_1[\"TP_moe\"] + down_moirai_up_moe_1[\"FP_moe\"])\n",
        "precision_moe_down_down = 100.0 * down_moirai_down_moe_1[\"TP_moe\"] / (down_moirai_down_moe_1[\"TP_moe\"] + down_moirai_down_moe_1[\"FP_moe\"])\n",
        "\n",
        "# Precision matrix\n",
        "precision_matrix = np.array([\n",
        "    [precision_moe_up_up, precision_moe_up_down],   # Moirai Up\n",
        "    [precision_moe_down_up, precision_moe_down_down] # Moirai Down\n",
        "])\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(precision_matrix, annot=True, fmt=\".2f\", cmap='Blues',\n",
        "            xticklabels=['Time-Moe Up', 'Time-Moe Down'],\n",
        "            yticklabels=['Moirai Up', 'Moirai Down'])\n",
        "plt.title('Precision for Time-Moe (H=1)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yE5aC2deaS0C"
      },
      "source": [
        "# Buckets Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Model Precision Analysis by Feature Buckets**\n",
        "This notebook analyzes the precision of three different models (`moirai`, `chronos`, `time_moe`) by quantile buckets of selected features. The results are visualized in precision plots for each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# --- 1) Load model data (only once) ---\n",
        "df_moirai = load_model_data(\"moirai\")\n",
        "df_chronos = load_model_data(\"chronos\")\n",
        "df_time_moe = load_model_data(\"time_moe\")\n",
        "\n",
        "# --- 2) Define common parameters ---\n",
        "first_predicted_row, last_predicted_row = context_length_index, len(df_moirai) - prediction_length\n",
        "features = [\"ATR_10\", \"RSI\", \"DistanceToEMM20\", \"DistanceToEMM60\", \"DistanceToMM20\", \"DistanceToMM60\", \"Volume\"]\n",
        "\n",
        "# For plotting\n",
        "horizons = [0, 2, 11]  # H=1, H=3, H=12\n",
        "horizon_labels = [\"H=1\", \"H=3\", \"H=12\"]\n",
        "horizon_styles = ['-', '--', ':']  # solid, dashed, dotted\n",
        "model_colors = {\"moirai\": \"blue\", \"chronos\": \"red\", \"time_moe\": \"green\"}\n",
        "ventile_labels = [f\"{i*5}-{(i+1)*5}\" for i in range(20)]  # \"0-5\", \"5-10\", ..., \"95-100\"\n",
        "\n",
        "# --- 3) Loop over each feature ---\n",
        "for feature in features:\n",
        "    # --- 3a) Compute the ventiles for this feature (using moirai data) ---\n",
        "    feature_ventile = df_moirai[feature].quantile([i / 20 for i in range(1, 20)])\n",
        "    \n",
        "    # --- 3b) Create the data structure for each model ---\n",
        "    models = {\n",
        "        \"moirai\": {\"df\": df_moirai, \"quantile_data\": []},\n",
        "        \"chronos\": {\"df\": df_chronos, \"quantile_data\": []},\n",
        "        \"time_moe\": {\"df\": df_time_moe, \"quantile_data\": []}\n",
        "    }\n",
        "    \n",
        "    # --- 3c) Initialize quantile buckets for each model ---\n",
        "    for model_name, model_data in models.items():\n",
        "        for i, value in enumerate(feature_ventile):\n",
        "            model_data[\"quantile_data\"].append({\n",
        "                'quantile': i + 1,\n",
        "                'value': value,\n",
        "                'TP_list': [0] * 12,  # 12 horizons\n",
        "                'FP_list': [0] * 12\n",
        "            })\n",
        "        # Add last bucket for values above the highest ventile\n",
        "        model_data[\"quantile_data\"].append({\n",
        "            'quantile': 20,\n",
        "            'value': float('inf'),\n",
        "            'TP_list': [0] * 12,\n",
        "            'FP_list': [0] * 12\n",
        "        })\n",
        "    \n",
        "    # --- 3d) Fill the quantile buckets with TP/FP values for each model ---\n",
        "    for model_name, model_data in models.items():\n",
        "        df_model = model_data[\"df\"]\n",
        "        \n",
        "        # Iterate over the predicted rows only\n",
        "        for index, row in df_model[first_predicted_row:last_predicted_row].iterrows():\n",
        "            feature_value = row[feature]\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            \n",
        "            # Find which quantile bucket this row's feature_value belongs to\n",
        "            for q in model_data[\"quantile_data\"]:\n",
        "                if feature_value <= q['value']:\n",
        "                    for i in range(12):  # 12 horizons\n",
        "                        q['TP_list'][i] += TP_list[i]\n",
        "                        q['FP_list'][i] += FP_list[i]\n",
        "                    break  # Stop after assigning to the first matching bucket\n",
        "    \n",
        "    # --- 3e) Plot precision by quantile bucket for each model and horizon ---\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    for model_name, model_data in models.items():\n",
        "        for h_idx, horizon in enumerate(horizons):\n",
        "            precision_values = []\n",
        "            for q in model_data[\"quantile_data\"]:\n",
        "                TP = q['TP_list'][horizon]\n",
        "                FP = q['FP_list'][horizon]\n",
        "                precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "                precision_values.append(precision)\n",
        "            \n",
        "            # Plot precision values\n",
        "            plt.plot(\n",
        "                range(len(precision_values)),\n",
        "                precision_values,\n",
        "                color=model_colors[model_name],\n",
        "                linestyle=horizon_styles[h_idx],\n",
        "                label=f\"{model_name} {horizon_labels[h_idx]}\"\n",
        "            )\n",
        "    \n",
        "    # Add horizontal lines for reference (optional)\n",
        "    plt.axhline(y=0.7, color='black', linestyle='--', alpha=0.7, label='Precision = 0.7')\n",
        "    plt.axhline(y=0.8, color='black', linestyle='--', alpha=0.7, label='Precision = 0.8')\n",
        "    \n",
        "    # Set x-axis labels to ventile ranges\n",
        "    plt.xticks(range(20), ventile_labels, rotation=45)\n",
        "    plt.ylim(0.4, 1)\n",
        "    \n",
        "    plot_title = f\"ES Future - Precision per Buckets for {feature}\"\n",
        "    plt.xlabel('Feature Ventiles (%)')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(plot_title)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    file_path = f\"analysis/bucket_analysis_plots/temporary_buckets/{plot_title}.png\"\n",
        "    plt.savefig(file_path, dpi=300, bbox_inches=\"tight\")\n",
        "    Image.open(file_path).save(file_path, format=\"PNG\", optimize=True, compress_level=9)\n",
        "    \n",
        "    # Show the plot for the current feature\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bucket analysis for H=1, for the three Models and for all features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code is very similar with the one above. For clarity, the two codes are separated because the first one plot 7 graphs (one for each feature) with all three models on it. While this part focuses on plotting 3 graphs (one for each models) by fixing H=1 and the model. The two logics are different and need two separate code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# 1) Load the data for all models\n",
        "df_moirai = load_model_data(\"moirai\")\n",
        "df_chronos = load_model_data(\"chronos\")\n",
        "df_time_moe = load_model_data(\"time_moe\")\n",
        "\n",
        "# 2) Define parameters\n",
        "first_predicted_row, last_predicted_row = 383, len(df_moirai) - 12\n",
        "features = [\"ATR_10\", \"RSI\", \"Volume\", \"DistanceToEMM20\", \n",
        "            \"DistanceToEMM60\", \"DistanceToMM20\", \"DistanceToMM60\"]\n",
        "\n",
        "# We'll focus on H=1, which is index 0 in TP_list/FP_list\n",
        "horizon_index = 0\n",
        "\n",
        "# Keep a dictionary of model names to their DataFrame\n",
        "models = {\n",
        "    \"moirai\": df_moirai,\n",
        "    \"chronos\": df_chronos,\n",
        "    \"time_moe\": df_time_moe\n",
        "}\n",
        "\n",
        "# Each feature will get its own color\n",
        "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
        "\n",
        "# For labeling x-axis with ventiles\n",
        "ventile_labels = [f\"{i*5}-{(i+1)*5}\" for i in range(20)]\n",
        "\n",
        "# 3) Create one figure per model\n",
        "for model_name, df_model in models.items():\n",
        "    \n",
        "    # Start a new figure for this model\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    \n",
        "    # For each feature, we will compute ventiles (using moirai's data, \n",
        "    # as in your advanced code) and plot on this model's figure\n",
        "    for f_idx, feature in enumerate(features):\n",
        "        \n",
        "        # --- a) Compute ventiles for this feature (based on df_moirai) ---\n",
        "        feature_ventiles = df_moirai[feature].quantile([i / 20 for i in range(1, 20)])\n",
        "        \n",
        "        # --- b) Initialize the quantile buckets (20 buckets) ---\n",
        "        quantile_data = []\n",
        "        for q_val in feature_ventiles:\n",
        "            quantile_data.append({'value': q_val, 'TP': 0, 'FP': 0})\n",
        "        # One extra bucket for values above the 95th percentile\n",
        "        quantile_data.append({'value': float('inf'), 'TP': 0, 'FP': 0})\n",
        "        \n",
        "        # --- c) Fill each bucket with TP/FP for horizon=1 ---\n",
        "        #     using the current model's DataFrame\n",
        "        for index, row in df_model[first_predicted_row:last_predicted_row].iterrows():\n",
        "            feature_value = row[feature]\n",
        "            \n",
        "            # Convert the \"TP\" and \"FP\" column from string to list\n",
        "            TP_list = ast.literal_eval(row[\"TP\"])\n",
        "            FP_list = ast.literal_eval(row[\"FP\"])\n",
        "            \n",
        "            # Find the correct bucket for this feature_value\n",
        "            for bucket in quantile_data:\n",
        "                if feature_value <= bucket['value']:\n",
        "                    bucket['TP'] += TP_list[horizon_index]  # H=1\n",
        "                    bucket['FP'] += FP_list[horizon_index]  # H=1\n",
        "                    break\n",
        "        \n",
        "        # --- d) Compute precision for each bucket ---\n",
        "        precision_values = []\n",
        "        for bucket in quantile_data:\n",
        "            tp = bucket['TP']\n",
        "            fp = bucket['FP']\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "            precision_values.append(precision)\n",
        "        \n",
        "        # --- e) Plot the precision curve for this feature on this model ---\n",
        "        color = colors[f_idx]\n",
        "        label = f\"{feature} (H=1)\"\n",
        "        \n",
        "        plt.plot(\n",
        "            range(20),             # x-values: bucket indices 0..19\n",
        "            precision_values,      # y-values: computed precision\n",
        "            color=color,\n",
        "            label=label\n",
        "        )\n",
        "    \n",
        "    # 4) Finalize the plot for this model\n",
        "    plt.axhline(y=0.7, color='black', linestyle='--', alpha=0.7, label='Precision = 0.7')\n",
        "    plt.axhline(y=0.8, color='black', linestyle='--', alpha=0.7, label='Precision = 0.8')\n",
        "    \n",
        "    plt.xticks(range(20), ventile_labels, rotation=45)\n",
        "    plt.ylim(0.4, 1)\n",
        "    plt.xlabel('Feature Ventiles (%)')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f\"Precision per Buckets (H=1) for {model_name.capitalize()}\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.legend(loc='best', ncol=2)\n",
        "    plt.tight_layout()\n",
        "    \n",
        "\n",
        "    file_path = f\"../bucket_analysis_plots/temporary_buckets/{model_name}_H1_Precision_Buckets_AllFeatures.png\"\n",
        "    plt.savefig(file_path, dpi=300, bbox_inches=\"tight\")\n",
        "    Image.open(file_path).save(file_path, format=\"PNG\", optimize=True, compress_level=9)\n",
        "    \n",
        "    # Show the figure for the current model\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scatter plot DistanceToEMA20, DistanceToEMA60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "\n",
        "# Load the historical data\n",
        "df = pd.read_csv(\"data/ES=F.csv\", parse_dates=True, index_col=0)\n",
        "\n",
        "# Compute Pearson and Spearman correlation\n",
        "pearson_corr = df[['DistanceToEMM20', 'DistanceToEMM60']].corr(method='pearson')\n",
        "spearman_corr = df[['DistanceToEMM20', 'DistanceToEMM60']].corr(method='spearman')\n",
        "\n",
        "print(\"Pearson Correlation:\\n\", pearson_corr)\n",
        "print(\"Spearman Correlation:\\n\", spearman_corr)\n",
        "\n",
        "# Density Plot\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.kdeplot(data=df, x='DistanceToEMM20', y='DistanceToEMM60', fill=True, cmap=\"Blues\", levels=50)\n",
        "\n",
        "# Adding correlation coefficients as text\n",
        "plt.text(0.05, 0.95, f\"Pearson: {pearson_corr.iloc[0, 1]:.2f}\\nSpearman: {spearman_corr.iloc[0, 1]:.2f}\",\n",
        "         fontsize=12, transform=plt.gca().transAxes, verticalalignment='top', bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "title = \"Density Plot DistanceToEMA20 vs. DistanceToEMA60\"\n",
        "plt.title(title)\n",
        "\n",
        "out_file = f\"analysis/density_plot/temporary_density_plot/{title}.png\"\n",
        "plt.savefig(out_file, dpi=300, bbox_inches='tight')\n",
        "Image.open(out_file).save(out_file, format=\"PNG\", optimize=True, compress_level=9)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predictions comparison 2024-2025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "\n",
        "def plot_model_metrics(horizon_idx, start_idx=0, end_idx=None):\n",
        "    # Load models\n",
        "    df_moirai   = load_model_data(\"moirai\")\n",
        "    df_time_moe = load_model_data(\"time_moe\")\n",
        "    df_chronos  = load_model_data(\"chronos\")\n",
        "    \n",
        "    real_prices = []\n",
        "    pred_prices = {\"moirai\": [], \"time_moe\": [], \"chronos\": []}\n",
        "\n",
        "    # Vérifie qu'on peut les parcourir en parallèle\n",
        "    assert len(df_moirai) == len(df_time_moe), \"Les deux DataFrames doivent avoir la même taille.\"\n",
        "\n",
        "    first_predicted_row, last_predicted_row = context_length_index, len(df_moirai) - prediction_length\n",
        "\n",
        "    moirai_rows   = df_moirai[first_predicted_row:last_predicted_row].iterrows()\n",
        "    time_moe_rows = df_time_moe[first_predicted_row:last_predicted_row].iterrows()\n",
        "    chronos_rows  = df_chronos[first_predicted_row:last_predicted_row].iterrows()\n",
        "\n",
        "    current_price = 0\n",
        "    for i, ((index_moirai, row_moirai), (index_time_moe, row_time_moe), (index_chronos, row_chronos)) in enumerate(zip(moirai_rows, time_moe_rows, chronos_rows)):\n",
        "        if i == 0:\n",
        "            current_price = float(row_moirai[\"Close\"])\n",
        "            continue\n",
        "        real_prices.append(current_price)\n",
        "        pred_prices[\"moirai\"].append(ast.literal_eval(row_moirai[\"Result\"])[horizon_idx])\n",
        "        pred_prices[\"time_moe\"].append(ast.literal_eval(row_time_moe[\"Result\"])[horizon_idx])\n",
        "        pred_prices[\"chronos\"].append(ast.literal_eval(row_chronos[\"Result\"])[horizon_idx])\n",
        "        \n",
        "        current_price = float(row_moirai[\"Close\"])\n",
        "\n",
        "    # Select zoomed period\n",
        "    if end_idx is None:\n",
        "        end_idx = len(real_prices)\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(real_prices[start_idx:end_idx], label='Real Prices', color='black', linewidth=2)\n",
        "    plt.plot(pred_prices[\"moirai\"][start_idx:end_idx], label='Moirai Predictions', linestyle='--')\n",
        "    plt.plot(pred_prices[\"time_moe\"][start_idx:end_idx], label='Time MoE Predictions', linestyle='--')\n",
        "    plt.plot(pred_prices[\"chronos\"][start_idx:end_idx], label='Chronos Predictions', linestyle='--')\n",
        "\n",
        "    plt.title(f'Model Predictions vs Real Prices (Horizon {horizon_idx})')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Price')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_model_metrics(0, start_idx=50, end_idx=70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "days = {\n",
        "    0: \"Monday\",\n",
        "    1: \"Tuesday\",\n",
        "    2: \"Wednesday\",\n",
        "    3: \"Thursday\",\n",
        "    4: \"Friday\",\n",
        "}\n",
        "\n",
        "hours = {\n",
        "    0: \"12AM\",  1: \"1AM\",  2: \"2AM\",  3: \"3AM\",  4: \"4AM\",  5: \"5AM\",\n",
        "    6: \"6AM\",  7: \"7AM\",  8: \"8AM\",  9: \"9AM\",  10: \"10AM\", 11: \"11AM\",\n",
        "    12: \"12PM\", 13: \"1PM\", 14: \"2PM\", 15: \"3PM\", 16: \"4PM\", 17: \"5PM\",\n",
        "    18: \"6PM\", 19: \"7PM\", 20: \"8PM\", 21: \"9PM\", 22: \"10PM\", 23: \"11PM\"\n",
        "}\n",
        "\n",
        "heatmap_day_week = {}\n",
        "\n",
        "for day in days.keys():\n",
        "    heatmap_day_week[day] = {}\n",
        "    for hour in hours.keys():\n",
        "        heatmap_day_week[day][hour] = {}  # Initialize this as a dictionary\n",
        "        for model_index, model_name in models.items():\n",
        "            heatmap_day_week[day][hour][model_name] = {\"values\": [], \"predictions\": [], \"APE\": 0}\n",
        "\n",
        "for model_index, model_name in models.items():\n",
        "    df_model = load_model_data(model_name).reset_index(drop=True)  # Reset index for positional access\n",
        "    model_rows = df_model.iloc[context_length_index:len(df_model)-prediction_length].iterrows()\n",
        "    first_predicted_row, last_predicted_row = context_length_index, len(df_model) - prediction_length\n",
        "    for i, (index_model, row_model) in enumerate(model_rows):\n",
        "    \n",
        "        day_index = row_model[\"Day\"]\n",
        "        hour_index = row_model[\"Hour\"]\n",
        "\n",
        "        if day_index < 5: # exclude week-ends\n",
        "        \n",
        "            results = ast.literal_eval(row_model[\"Result\"])\n",
        "            sub_df = df_model.iloc[index_model+1:index_model+prediction_length+1]\n",
        "            sub_df_values = sub_df[\"Close\"].values\n",
        "\n",
        "            if len(results) == len(sub_df_values):\n",
        "                count_total += 1\n",
        "                heatmap_day_week[day_index][hour_index][model_name][\"values\"].extend(sub_df_values)\n",
        "                heatmap_day_week[day_index][hour_index][model_name][\"predictions\"].extend(results)\n",
        "            else:\n",
        "                print(\"Mismatch in prediction and actual values length.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute APE for each day/hour/model\n",
        "for day in heatmap_day_week:\n",
        "    for hour in heatmap_day_week[day]:\n",
        "        for model_name in heatmap_day_week[day][hour]:\n",
        "            values = heatmap_day_week[day][hour][model_name][\"values\"]\n",
        "            predictions = heatmap_day_week[day][hour][model_name][\"predictions\"]\n",
        "            \n",
        "            if len(values) == len(predictions) and len(values) > 0:\n",
        "                ape_values = [\n",
        "                    abs(actual - predicted) / actual * 100\n",
        "                    for actual, predicted in zip(values, predictions)\n",
        "                    if actual != 0  # Avoid division by zero\n",
        "                ]\n",
        "                heatmap_day_week[day][hour][model_name][\"APE\"] = (\n",
        "                    sum(ape_values) / len(ape_values) if ape_values else 0\n",
        "                )\n",
        "                print(\"APE: \", heatmap_day_week[day][hour][model_name][\"APE\"])\n",
        "            else:\n",
        "                heatmap_day_week[day][hour][model_name][\"APE\"] = None  # Handle empty/missing data\n",
        "\n",
        "for day in heatmap_day_week:\n",
        "    for hour in heatmap_day_week[day]:\n",
        "        for model_name in heatmap_day_week[day][hour]:\n",
        "            # print APE\n",
        "            print(f\"Day: {days[day]}, Hour: {hours[hour]}, Model: {model_name}, APE: {heatmap_day_week[day][hour][model_name]['APE']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Mapping for days and hours (as provided)\n",
        "days = {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\", 4: \"Friday\"}\n",
        "hours = {0: \"12AM\",  1: \"1AM\",  2: \"2AM\",  3: \"3AM\",  4: \"4AM\",  5: \"5AM\",\n",
        "         6: \"6AM\",  7: \"7AM\",  8: \"8AM\",  9: \"9AM\", 10: \"10AM\", 11: \"11AM\",\n",
        "         12: \"12PM\", 13: \"1PM\", 14: \"2PM\", 15: \"3PM\", 16: \"4PM\", 17: \"5PM\",\n",
        "         18: \"6PM\", 19: \"7PM\", 20: \"8PM\", 21: \"9PM\", 22: \"10PM\", 23: \"11PM\"}\n",
        "\n",
        "# Create a matrix to store the average APE for each hour (rows) and day (columns)\n",
        "heatmap_matrix = np.full((len(hours), len(days)), np.nan)\n",
        "\n",
        "# Iterate over days and hours in the pre-populated heatmap_day_week dictionary\n",
        "for day_index in days.keys():\n",
        "    for hour_index in hours.keys():\n",
        "        cell_data = heatmap_day_week[day_index][hour_index]\n",
        "        ape_values = []\n",
        "        # Gather APE values from each model\n",
        "        for model_name, data in cell_data.items():\n",
        "            ape = data[\"APE\"]\n",
        "            if ape is not None:\n",
        "                ape_values.append(ape)\n",
        "        # Compute the average APE if there is valid data\n",
        "        if ape_values:\n",
        "            avg_ape = sum(ape_values) / len(ape_values)\n",
        "            heatmap_matrix[hour_index, day_index] = avg_ape\n",
        "\n",
        "# Use only non-NaN values for normalization bounds\n",
        "valid_values = heatmap_matrix[~np.isnan(heatmap_matrix)]\n",
        "lower_bound = np.percentile(valid_values, 5)\n",
        "upper_bound = np.percentile(valid_values, 95)\n",
        "norm = plt.Normalize(lower_bound, upper_bound)\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "heatmap = plt.imshow(heatmap_matrix, cmap=plt.cm.RdYlGn_r, norm=norm, aspect='auto', origin='lower')\n",
        "\n",
        "# Create x and y labels from the provided mappings\n",
        "x_labels = [days[i] for i in sorted(days.keys())]\n",
        "y_labels = [hours[i] for i in sorted(hours.keys())]\n",
        "\n",
        "plt.xticks(ticks=np.arange(len(x_labels)), labels=x_labels, fontsize=12, rotation=45, ha='right')\n",
        "plt.yticks(ticks=np.arange(len(y_labels)), labels=y_labels, fontsize=12)\n",
        "plt.xlabel(\"Day\", fontsize=14)\n",
        "plt.ylabel(\"Hour\", fontsize=14)\n",
        "plt.title(\"Heatmap of Average APE by Day and Hour - 2023-2024\", fontsize=16)\n",
        "\n",
        "# Colorbar setup\n",
        "cbar = plt.colorbar(heatmap)\n",
        "cbar.set_label(\"Average APE\", fontsize=12)\n",
        "\n",
        "# Annotate each cell with its value (formatted to 3 decimal places)\n",
        "for i in range(heatmap_matrix.shape[0]):  # iterate over hours (rows)\n",
        "    for j in range(heatmap_matrix.shape[1]):  # iterate over days (columns)\n",
        "        value = heatmap_matrix[i, j]\n",
        "        if not np.isnan(value):\n",
        "            # Use white text if the normalized value is low, otherwise black\n",
        "            text_color = 'white' if norm(value) < 0.2 else 'black'\n",
        "            plt.text(j, i, f\"{value:.3f}\", ha='center', va='center', color=text_color, fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the heatmap image and then compress it\n",
        "output_path = \"analysis/heatmaps/temporary_heatmaps_volume/Heatmap_of_Average_APE.png\"\n",
        "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
        "Image.open(output_path).save(output_path, format=\"PNG\", optimize=True, compress_level=9)\n",
        "\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JNk9GmDiFGZd"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
